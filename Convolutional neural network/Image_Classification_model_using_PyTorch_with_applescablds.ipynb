{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification model using PyTorch with applescablds.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTQYccYnEr7SOynr9jNyJR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pythonuzgit/elmurodov/blob/master/Convolutional%20neural%20network/Image_Classification_model_using_PyTorch_with_applescablds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMKjcLGL6d8M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHdoLG0J6rKY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3HHh8LP6gGl",
        "outputId": "e122fb10-da8e-4e9d-c079-0567d9c85d3b"
      },
      "source": [
        "!kaggle datasets download -d projectlzp201910094/applescablds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading applescablds.zip to /content\n",
            "100% 3.21G/3.22G [00:56<00:00, 22.2MB/s]\n",
            "100% 3.22G/3.22G [00:56<00:00, 61.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO3kZOrd_Uqu"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import pathlib\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifr2AwKh8xST"
      },
      "source": [
        "Clean data an remove non images files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWVv74288vPQ"
      },
      "source": [
        "path  = '/content/AppleScabLDs'\n",
        "\n",
        "for folder in os.listdir(path):\n",
        "  for img_file in os.listdir(os.path.join(path, folder)):\n",
        "    img_file = os.path.join(path, folder, img_file)\n",
        "\n",
        "    try:\n",
        "      img = Image.open(img_file)\n",
        "      if img.mode != 'RGB':\n",
        "        os.remove(img_file)\n",
        "\n",
        "    except:\n",
        "      os.remove(img_file)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1eZOzV6NVTW"
      },
      "source": [
        "Data Preprocessing and convert to tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sAWDufJ_RkW"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.Resize(255),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.5],\n",
        "                                                     [0.5])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder('/content/AppleScabLDs', transform = transform)\n",
        "\n",
        "dataset_len = len(dataset)\n",
        "\n",
        "train_len, test_len = dataset_len - 300, 300\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_len, test_len])\n",
        "\n",
        "batch_size = 200\n",
        "\n",
        "#train and test dataloader\n",
        "train_set = DataLoader(dataset = train_set, shuffle = True, batch_size = batch_size)\n",
        "test_set = DataLoader(dataset = test_set, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Z-Z4WdT8MZ"
      },
      "source": [
        "Checking device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW-8Gd4LThZQ"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEXTR97bWmJZ",
        "outputId": "1ed8b2d6-a8fb-409a-d928-99b74e8fda85"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z__UTY-ca834"
      },
      "source": [
        "Exploring Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-Vpr0bibA7q",
        "outputId": "9b1e09f3-fe7d-4ac5-9e0c-6cece5cf53c8"
      },
      "source": [
        "classes = os.listdir(path)\n",
        "\n",
        "print('Total classes:', len(classes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total classes: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3nTzQZGsykv"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2VXMcP8dPn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3f70a5-9b3a-4c33-cada-7cd52df6ba20"
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.dropout = nn.Dropout(p = 0.2)\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 6, kernel_size = 4)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 4)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 12, out_channels = 14, kernel_size = 4)\n",
        "    self.conv4 = nn.Conv2d(in_channels = 14, out_channels = 16, kernel_size = 4)\n",
        "    self.conv5 = nn.Conv2d(in_channels = 16, out_channels = 20, kernel_size = 4)\n",
        "\n",
        "    \n",
        "    self.fc1 = nn.Linear(in_features = 20 * 4 * 4, out_features = 250)\n",
        "    self.fc2 = nn.Linear(in_features = 250, out_features = 200)\n",
        "    self.fc3 = nn.Linear(in_features = 200, out_features = 50)\n",
        "    self.fc4 = nn.Linear(in_features = 50, out_features = 10)\n",
        "    self.fc5 = nn.Linear(in_features = 10, out_features = 5)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.pool(F.relu(self.conv3(x)))\n",
        "    x = self.pool(F.relu(self.conv4(x)))\n",
        "    x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "\n",
        "\n",
        "    x = x.reshape(-1, 20 * 4 * 4)\n",
        "    x = self.dropout(F.relu(self.fc1(x)))\n",
        "    x = self.dropout(F.relu(self.fc2(x)))\n",
        "    x = self.dropout(F.relu(self.fc3(x)))\n",
        "    x = self.dropout(F.relu(self.fc4(x)))\n",
        "    x = self.fc5(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "net = Model().to(device)\n",
        "\n",
        "print(net)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (conv3): Conv2d(12, 14, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (conv4): Conv2d(14, 16, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (conv5): Conv2d(16, 20, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=320, out_features=250, bias=True)\n",
            "  (fc2): Linear(in_features=250, out_features=200, bias=True)\n",
            "  (fc3): Linear(in_features=200, out_features=50, bias=True)\n",
            "  (fc4): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (fc5): Linear(in_features=10, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8yE_J6t83_"
      },
      "source": [
        "Define Loss and optimizer functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8-otTEedPbC"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = 0.001, weight_decay = 1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zekj1GHMvG-V"
      },
      "source": [
        "Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sgw0gqzvDsN",
        "outputId": "e5a9aee4-1f29-462e-cd94-8ef73f4da0c2"
      },
      "source": [
        "net.train()\n",
        "\n",
        "for epoch in range(25):\n",
        "  total_correct = 0.0\n",
        "  running_loss = 0.0\n",
        "  for i, (inputs, labels) in enumerate(train_set):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output = net(inputs)\n",
        "    output_idx = torch.argmax(output, dim = 1)\n",
        "    total_correct += (labels == output_idx).sum().item()\n",
        "    optimizer.zero_grad()\n",
        "    loss = creterion(output, labels)\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "  print(f'Epoch: {epoch}  Loss: {running_loss/train_len}  Accuracy: {(total_correct/train_len) * 100}%') \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0  Loss: 1.619990551069881  Accuracy: 17.865168539325843%\n",
            "Epoch: 1  Loss: 1.6036869461616774  Accuracy: 17.865168539325843%\n",
            "Epoch: 2  Loss: 1.4058881020278073  Accuracy: 29.55056179775281%\n",
            "Epoch: 3  Loss: 0.8208612712581506  Accuracy: 74.8314606741573%\n",
            "Epoch: 4  Loss: 0.6765493978275342  Accuracy: 74.49438202247191%\n",
            "Epoch: 5  Loss: 0.6616781220007478  Accuracy: 70.4494382022472%\n",
            "Epoch: 6  Loss: 0.6111323733008309  Accuracy: 76.17977528089888%\n",
            "Epoch: 7  Loss: 0.5945323940073506  Accuracy: 79.5505617977528%\n",
            "Epoch: 8  Loss: 0.5962608188725589  Accuracy: 80.2247191011236%\n",
            "Epoch: 9  Loss: 0.5885730137985744  Accuracy: 80.4494382022472%\n",
            "Epoch: 10  Loss: 0.5355398902732335  Accuracy: 80.11235955056179%\n",
            "Epoch: 11  Loss: 0.5779945903949524  Accuracy: 79.7752808988764%\n",
            "Epoch: 12  Loss: 0.5370925423804294  Accuracy: 81.12359550561797%\n",
            "Epoch: 13  Loss: 0.5404501607578792  Accuracy: 80.67415730337079%\n",
            "Epoch: 14  Loss: 0.5476463028554166  Accuracy: 80.4494382022472%\n",
            "Epoch: 15  Loss: 0.5279752806331335  Accuracy: 80.0%\n",
            "Epoch: 16  Loss: 0.5304497487089607  Accuracy: 80.2247191011236%\n",
            "Epoch: 17  Loss: 0.519274225395717  Accuracy: 81.12359550561797%\n",
            "Epoch: 18  Loss: 0.5174040261949047  Accuracy: 80.78651685393258%\n",
            "Epoch: 19  Loss: 0.5226223596026388  Accuracy: 81.34831460674158%\n",
            "Epoch: 20  Loss: 0.5125285872582639  Accuracy: 81.46067415730337%\n",
            "Epoch: 21  Loss: 0.5031304362784611  Accuracy: 81.23595505617978%\n",
            "Epoch: 22  Loss: 0.493102131264933  Accuracy: 80.78651685393258%\n",
            "Epoch: 23  Loss: 0.4833693906162562  Accuracy: 81.57303370786518%\n",
            "Epoch: 24  Loss: 0.4910211569807503  Accuracy: 81.46067415730337%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}